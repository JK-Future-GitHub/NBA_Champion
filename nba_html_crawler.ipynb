{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pngs/nba_html_crawler.png)\n",
    "1. Define Foundation\n",
    "2. Get Player Alphabet Urls\n",
    "3. Get Player Urls\n",
    "4. Save Player Html Pages\n",
    "5. Get Award Pages\n",
    "6. Save Award Html Pages\n",
    "7. Get ALL Team Season Urls\n",
    "8. Save ALL Team Season Html Pages \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ➤ 1 Define Foundation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time as t\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAIN_URL = r\"https://www.basketball-reference.com/\"\n",
    "ALPHABET_URL = r\"https://www.basketball-reference.com/players/\"\n",
    "ALL_NBA_URL = r\"https://www.basketball-reference.com/awards/all_league.html\"\n",
    "ALL_DEFENSIVE_URL= r\"https://www.basketball-reference.com/awards/all_defense.html\"\n",
    "MVP_URL = r\"https://www.basketball-reference.com/awards/mvp.html\"\n",
    "SEASON_URL = r\"https://www.basketball-reference.com/leagues/\"\n",
    "\n",
    "DATA_PATH = r\"C:\\Users\\knaue\\Documents\\Data\\NBA\"\n",
    "PLAYER_HTML_PATH = os.path.join(DATA_PATH, \"PLAYER_HTML\")\n",
    "AWARD_HTML_PATH = os.path.join(DATA_PATH, \"AWARD_HTML\")\n",
    "SEASON_HTML_PATH = os.path.join(DATA_PATH, \"SEASON_HTML\")\n",
    "\n",
    "ALPHABET_PATH = os.path.join(DATA_PATH, \"Alphabet_Urls.csv\")\n",
    "PLAYER_PATH = os.path.join(DATA_PATH, \"Player_Urls.csv\")\n",
    "AWARD_PATH = os.path.join(DATA_PATH, \"Award_Urls.csv\")\n",
    "SEASON_PATH = os.path.join(DATA_PATH, \"Season_Urls.csv\")\n",
    "\n",
    "PARSER = 'lxml'\n",
    "ONLY_ACTIVE_PLAYER = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_out_comment(soup: BeautifulSoup) -> BeautifulSoup:\n",
    "    content = str(soup).replace('<!--', '')\n",
    "    content = content.replace('-->', '')\n",
    "    return BeautifulSoup(content, PARSER)\n",
    "\n",
    "def request_data(url: str, sleep_time_sec: float = 1.0, with_comment: bool = True) -> BeautifulSoup:\n",
    "    t.sleep(sleep_time_sec)\n",
    "    \n",
    "    if with_comment: \n",
    "        return BeautifulSoup(requests.get(url).content, PARSER)\n",
    "    return filter_out_comment(BeautifulSoup(requests.get(url).content, PARSER))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def season_to_int(cell_value: str):\n",
    "    if cell_value[-2:] == \"00\":\n",
    "        return (int(cell_value[:2]) + 1)*100\n",
    "    else:\n",
    "        return int(cell_value[:2] + cell_value[-2:])   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ➤ 2 Get Player Alphabet Urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to:  C:\\Users\\knaue\\Documents\\Data\\NBA\\Alphabet_Urls.csv\n"
     ]
    }
   ],
   "source": [
    "content = request_data(ALPHABET_URL, 1.0, False)\n",
    "content = content.find(\"div\", id=\"div_alphabet\")\n",
    "\n",
    "alphabet_dict = {tag.get_text(): tag['href'] for tag in content.find_all(\"a\")}\n",
    "alphabet_dict = {key: urljoin(ALPHABET_URL, value) for key, value in alphabet_dict.items()}\n",
    "\n",
    "df_alphabet_urls = pd.DataFrame.from_dict(alphabet_dict, orient=\"index\", columns=[\"url\"])\n",
    "df_alphabet_urls.to_csv(ALPHABET_PATH, encoding=\"utf-8-sig\")\n",
    "print(\"Saved to: \", ALPHABET_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ➤ 3 Get Player Urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25...\n",
      "Saved to:  C:\\Users\\knaue\\Documents\\Data\\NBA\\Player_Urls.csv\n"
     ]
    }
   ],
   "source": [
    "df_alphabet_urls = pd.read_csv(ALPHABET_PATH, encoding=\"utf-8-sig\")\n",
    "dfs = []\n",
    "i = 0\n",
    "\n",
    "for url in df_alphabet_urls[\"url\"].values: \n",
    "    i += 1\n",
    "    sys.stdout.write(f\"\\r{i}/{len(df_alphabet_urls)}...\")\n",
    "    \n",
    "    content = request_data(url, 4.0, False)\n",
    "    content = content.find(\"table\", id=\"players\")\n",
    "    df = pd.read_html(str(content))[0]\n",
    "    \n",
    "    df['Hall_of_Fame'] = df['Player'].str.contains(\"\\*\")\n",
    "    df['Player'] = df['Player'].str.replace(\"\\*\", \"\", regex=True)\n",
    "    \n",
    "    all_as = [a for a in content.find_all(\"a\") if \"players\" in a['href']]\n",
    "    all_as = [ [urljoin(ALPHABET_URL, a['href']) , True] if a.previous_element.name == \"strong\" else [urljoin(ALPHABET_URL, a['href']), False] \n",
    "             for a in all_as]\n",
    "\n",
    "    df['Active'] = [is_active[-1] for is_active in all_as]\n",
    "    df['Url'] = [is_active[0] for is_active in all_as]\n",
    "    \n",
    "    dfs.append(df)\n",
    "\n",
    "dfs = pd.concat(dfs, ignore_index=True)\n",
    "dfs['Path'] = dfs['Url'].apply(lambda cell: os.path.join(PLAYER_HTML_PATH, cell.replace(\"/\", \"{\").replace(\":\", \"}\")))\n",
    "dfs.to_csv(PLAYER_PATH, index=False, encoding=\"utf-8-sig\")\n",
    "print(\"\\nSaved to: \", PLAYER_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ➤ 4 Save Player Html Pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "687/687...\n",
      "Saved to:  C:\\Users\\knaue\\Documents\\Data\\NBA\\PLAYER_HTML ...\n"
     ]
    }
   ],
   "source": [
    "df_player_urls = pd.read_csv(PLAYER_PATH, usecols=[\"Active\", \"Url\"], encoding=\"utf-8-sig\")\n",
    "\n",
    "if ONLY_ACTIVE_PLAYER != None:\n",
    "    df_player_urls = df_player_urls[df_player_urls['Active'] == ONLY_ACTIVE_PLAYER] \n",
    "\n",
    "i = 0\n",
    "for active, url in df_player_urls[[\"Active\", \"Url\"]].values:\n",
    "    i += 1\n",
    "    sys.stdout.write(f\"\\r{i}/{len(df_player_urls)}...\")\n",
    "    \n",
    "    content = request_data(url=url, sleep_time_sec=2.0, with_comment=False)\n",
    "    url = url.replace(\"/\", \"{\").replace(\":\", \"}\")\n",
    "    player_path = os.path.join(PLAYER_HTML_PATH, url)\n",
    "\n",
    "    with open(player_path, \"w\", encoding='utf-8-sig') as f:\n",
    "        f.write(str(content))\n",
    "        f.close()\n",
    "        \n",
    "print(\"\\nSaved to: \", PLAYER_HTML_PATH, \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ➤ 5 Get Award Pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to:  C:\\Users\\knaue\\Documents\\Data\\NBA\\Award_Urls.csv\n"
     ]
    }
   ],
   "source": [
    "# MVP Voting\n",
    "content = request_data(url=MVP_URL, sleep_time_sec=4.0, with_comment=False)\n",
    "table = content.find(\"table\", id=\"mvp_NBA\")\n",
    "df_table = pd.read_html(str(table))[0]\n",
    "df_table = df_table.droplevel(0, axis=1)\n",
    "df_table['Season'] = df_table['Season'].apply(lambda x: season_to_int(x))\n",
    "\n",
    "votings = []\n",
    "for td in table.find(\"tbody\").findAll(\"td\", class_=\"center\", attrs={\"data-stat\":\"voting\"}):\n",
    "    votings.append(urljoin(MAIN_URL, td.a['href']))\n",
    "df_table.insert(loc=len(df_table.columns), column='Voting_Url', value=votings)\n",
    "\n",
    "df_table = df_table[['Season', 'Voting_Url']]\n",
    "df_table['Voting_Path'] = df_table['Voting_Url'].apply(lambda cell: os.path.join(AWARD_HTML_PATH, cell.replace(\"/\", \"{\").replace(\":\", \"}\")))\n",
    "\n",
    "# All NBA\n",
    "df_table.loc[len(df_table)] = [\"All_NBA\", ALL_NBA_URL, os.path.join(AWARD_HTML_PATH, ALL_NBA_URL.replace(\"/\", \"{\").replace(\":\", \"}\"))]\n",
    "\n",
    "# All Defensive\n",
    "df_table.loc[len(df_table)] = [\"All_DEFENSIVE\", ALL_DEFENSIVE_URL, os.path.join(AWARD_HTML_PATH, ALL_DEFENSIVE_URL.replace(\"/\", \"{\").replace(\":\", \"}\"))]\n",
    "\n",
    "df_table.to_csv(AWARD_PATH, index=False, encoding=\"utf-8-sig\")\n",
    "print(\"Saved to: \", AWARD_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ➤ 6 Save Award Html Pages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69/69...\n",
      "Saved to:  C:\\Users\\knaue\\Documents\\Data\\NBA\\AWARD_HTML ...\n"
     ]
    }
   ],
   "source": [
    "df_award = pd.read_csv(AWARD_PATH, encoding=\"utf-8-sig\")\n",
    "i = 0\n",
    "\n",
    "for url, path in df_award[['Voting_Url', 'Voting_Path']].values:\n",
    "    i += 1\n",
    "    sys.stdout.write(f\"\\r{i}/{len(df_award)}...\")\n",
    "    \n",
    "    content = request_data(url=url, sleep_time_sec=4.0, with_comment=False)\n",
    "    with open(path, \"w\", encoding='utf-8-sig') as f:\n",
    "        f.write(str(content))\n",
    "        f.close()\n",
    "        \n",
    "print(\"\\nSaved to: \", AWARD_HTML_PATH, \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ➤ 7 Get ALL Team Season Urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to:  C:\\Users\\knaue\\Documents\\Data\\NBA\\Season_Urls.csv\n"
     ]
    }
   ],
   "source": [
    "content = request_data(SEASON_URL, with_comment=False)\n",
    "content = content.find(\"table\", id=\"stats\")\n",
    "df = pd.read_html(str(content))[0]\n",
    "df = df.droplevel(0, axis=1)\n",
    "df.drop(['MVP', 'Rookie of the Year', 'Points', 'Rebounds', 'Assists', 'Win Shares'], axis=\"columns\", inplace=True)\n",
    "df = df[df['Lg'] == 'NBA']\n",
    "\n",
    "seasons = []\n",
    "for season in df['Season'].values:\n",
    "    season = content.find(text=season)\n",
    "    seasons.append(urljoin(SEASON_URL, season.parent['href']))\n",
    "\n",
    "df['Url_Season_Summary'] = seasons  \n",
    "df['Url_Season_Standings'] = df['Url_Season_Summary'].apply(lambda cell: cell[:-len(\".html\")] + \"_standings.html\")\n",
    "df['Url_Playoff_Standings'] = df['Url_Season_Standings'].str.replace(\"leagues\", \"playoffs\")\n",
    "df['Season'] = df['Season'].apply(lambda x: season_to_int(x))    \n",
    "    \n",
    "df.to_csv(SEASON_PATH, index=False, encoding=\"utf-8-sig\")\n",
    "print(\"Saved to: \", SEASON_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ➤ 8 Save ALL Team Season Html Pages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "222/222...\n",
      "Saved to:  C:\\Users\\knaue\\Documents\\Data\\NBA\\SEASON_HTML ...\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(SEASON_PATH, usecols=['Url_Season_Summary', 'Url_Season_Standings', 'Url_Playoff_Standings'], encoding=\"utf-8-sig\")\n",
    "i = 0\n",
    "\n",
    "unique_url_season_sum = df['Url_Season_Summary'].unique()\n",
    "unique_url_season_sta = df['Url_Season_Standings'].unique()\n",
    "unique_url_playoff_sta = df['Url_Playoff_Standings'].unique()\n",
    "unique_urls = np.concatenate((unique_url_season_sum, unique_url_season_sta, unique_url_playoff_sta), axis=0)\n",
    "\n",
    "for url in unique_urls:\n",
    "    i += 1\n",
    "    sys.stdout.write(f\"\\r{i}/{len(unique_urls)}...\")\n",
    "    \n",
    "    content = request_data(url=url, sleep_time_sec=2.0, with_comment=False)\n",
    "    url = url.replace(\"/\", \"{\").replace(\":\", \"}\")                           # Cant be saved the slash and : -> convert in /={ and :=}  \n",
    "    season_path = os.path.join(SEASON_HTML_PATH, url)\n",
    "    \n",
    "    with open(season_path, \"w\", encoding='utf-8-sig') as f:\n",
    "        f.write(str(content))\n",
    "        f.close()\n",
    "\n",
    "print(\"\\nSaved to: \", SEASON_HTML_PATH, \"...\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aa6708a3e9a8fadf3ed03c473ecc2d2a3bf5cea3ad7526930f095379c19fd7a0"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
